{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Optimization Analysis\n",
    "### data source: In conjunction with a volunteer [HackforLA](https://www.hackforla.org/) civic project this public Los Angeles parking [ticket data](https://data.lacity.org/Transportation/Parking-Citations/4f5p-udkv/about_data) was sourced from an API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import http.client\n",
    "import requests\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession,types\n",
    "from pyspark import SQLContext,SparkContext,SparkConf\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Configurations \n",
    "conf=SparkConf()\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "conf.set(\"spark.driver.memory\", \"4g\")\n",
    "conf.set(\"spark.cores.max\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Staring Spark Session\n",
    "# sc = SparkContext.getOrCreate(conf)\n",
    "# spark = SQLContext(sc)\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"ReadCSVFile\") \\\n",
    "    .config(\"spark.jars.packages\",\"org.postgresql:postgresql:42.6.0\")\\\n",
    "    .getOrCreate()\n",
    "sqlContext= SparkSession(spark)\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark.sparkContext.getConf().getAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in file\n",
    "file_path = 'Parking_Citations.csv'\n",
    "df_spark = spark.read.csv(file_path, header=True)\n",
    "\n",
    "# Write raw table to Postgres\n",
    "df_spark.write.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://spark-postgres-1:5432/sparkdb\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", 'public.\"luck-parking-raw\"') \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"postgres\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Validation and Exploration\n",
    "df_spark.createOrReplaceTempView(\"view\")\n",
    "\n",
    "# Row count\n",
    "count = spark.sql(\n",
    "    \"SELECT COUNT(*) from view\"\n",
    ")\n",
    "count.show()\n",
    "\n",
    "# Fields\n",
    "column_names = spark.sql(\n",
    "    \"SELECT * FROM view\"\n",
    ").schema.names\n",
    "\n",
    "column_names.show()\n",
    "\n",
    "# Distinct ID count\n",
    "distinct_id_count = spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT ticket_number)\n",
    "    FROM view \n",
    "\"\"\")\n",
    "distinct_id_count.show()\n",
    "\n",
    "# Checking for ID dups\n",
    "finding_dubs = spark.sql(\"\"\"\n",
    "    SELECT count(*) FROM\n",
    "    (SELECT ticket_number, COUNT(*) as dups\n",
    "    FROM view\n",
    "    GROUP BY ticket_number)\n",
    "    WHERE dups = 1\n",
    "\"\"\")\n",
    "finding_dubs.show()\n",
    "\n",
    "# High level anomolies\n",
    "res = spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM view\n",
    "    WHERE substring(issue_date,7,5) >= '2016' \n",
    "    AND meter_id=37\n",
    "    ORDER BY fine_amount DESC    \n",
    "\"\"\").toPandas()\n",
    "res\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
